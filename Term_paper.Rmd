---
title: "Spherelets"
subtitle: "Stat 185 Term Paper"
author: "Caleb Ren"
date: "December 14, 2019"
output: 
  pdf_document:
    extra_dependencies: ['algorithm', 'algorithmic']
    number_sections: true
    toc_depth: 4
    toc: true
    fig_caption: yes
geometry: margin = 3cm
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=T, message=F, warning=F,
                      fig.height=3, fig.align='center', fig.pos="H")
set.seed(185)
require(scatterplot3d)
require(kableExtra)
require(scales)
source('spca_functions.R')
```

\newpage

\section{Introduction}

Data that exists in a high-dimensional ambient space may instead be considered to lie near a lower dimensional manifold (e.g. a circle in $\mathbb{R}^2$ that lies ambiently in $\mathbb{R}^3$ space). Many techniques are focused on reducing the ambient space to one closer to the intrinsic space such as clustering [@duda2000], data compression [@hurtado2012], and predictive modelling [@lee2008]. Many of these techniques that attempt to approximate manifolds embedded in a lower dimensional space are locally linear, use multiscale dictionaries, and as such struggle with areas with high Gaussian curvature. Additionally, many techniques that approximate local manifolds do not perform well with out-of-sample error as they do not reveal much information on the traits of the lower dimension manifold.

A simple alternative that is able to handle curvature as well as out-of-sample error is to use pieces of (hyper)spheres to locally approximate the unknown subspace [@li2019]. This method was first proposed by Li and Dunson, who termed the technique spherical PCA (SPCA) [@li2017], or *spherelets* for short. Whereas principal component analysis (PCA) is an eigenvalue/eigenvector problem from an inherently *linear* dimension reduction problem, spherelets rely on projection mappings to the surfaces of a hypersphere as an underlying mechanism. As such, in simple cases of SPCA, closed form analytic solutions exist and can be seen as a generalization of PCA that is able to incorporate degrees of curvature. Spherical PCA in general does not have a closed-form solution due to the myriad ways in which a particular dataset can be partitioned and fit with sub-manifolds. Various algorithms exist to best partition the original dataset such as cover trees [@beygelzimer2006], METIS [@karypis1998], and iterated PCA [@szlam2009]. The type of partitioning done is context-dependent and multiple types of subsetting can be done to cross-validated to reduce test MSE. As examples, we will examine two types of partitioning: naively---by splitting the dataset into equal-sized clusters---and via partioning by iterated PCA [@li2019].

Data that is represented in higher dimension $\mathbb{R}^D$ can be projected down to a manifold in $\mathbb{R}^d$ consisting of a set of spherelets in $\mathbb{R}^d$. Ultimately, the algorithm partitions the dataset into $k$ subsets, each of which is fit with a submanifold $M_k \in \mathbb{R}^d$ with a corresponding projection map $\Psi_k: \mathbb{R}^D \rightarrow \mathbb{R}^d$. Given a dataset $\mathbf{X} \in \mathbb{R}^D$, the spherical PCA algorithm returns manifold $M$ and the projection map $\Psi$, which are the manifold and mapping from the data to the approximate manifold in $\mathbb{R}^d$, respectively.

Manifold approximation for model-building is one useful application of spherelets but the technique can be used for other purposes. Spherical PCA can also be used to denoise manifolds and generate visualizations of data in higher dimensions. Spherelets have been shown to outperform competiting denoising algorithms like Gaussian Blurring Mean Shift (GBMS), Manifold Blurring Mean Shift (MBMS), and Local Tangent Projection (LTP). Visualization techniques that preserve local geometry like Isomap, Locally Linear Embedding (LLE), and t-distributed Stochastic Neighborhood Embedding (t-SNE) can also be modified to incorporate spherical geometry using the same projection to spherical affine subspace, as in the original spherelets paper [@li2019].

In spherical PCA, a point $\vec{y}_i$ is projected down to a sphere $S_V(c, r)$, where $c$ is the center of the sphere, $r$ is the radius, and $V$ specifies the subspace on which the sphere lies. The optimal estimates for the sphere $S_{\hat{V}}(\hat{c}, \hat{r})$ are given by:

$$
\begin{gathered}
\hat{V} = (\vec{v}_1, \dots, \vec{v}_{d + 1}) \\
\hat{c} = -\frac{\vec{\eta}^*}{2}  \\
\hat{r} = \frac{1}{N} \sum_{k = 1}^N \left\|\vec{z}_i - \hat{c} \right\|
\end{gathered}
$$

Where $\vec{v}_i$ is the $i$th eigenvector of the covariance matrix times  $\mathbf{\Sigma}$, $\vec{z}_i$ is the the projected data down to the subspace and $\vec{\eta}^*$ is a vector based on the entire projected dataset $\mathbf{Z}$. After an introduction into notation of spherical PCA, I will introduce relevant theory, including the Main Theorem that undergirds spherical PCA in the following section, as well as assumptions made. I will discuss strengths and weaknesses of the algorithm in Section 3, and then provide numerical examples demonstrating areas in which spherelets perform well and where the algorithm struggles in Section 4.

\section{Method}

\subsection{Theory}

\subsubsection{Covering Number}

The Main Theorem underlying spherical PCA asserts that the covering number of spherelets is lower than that of an analogous PCA method using hyperplanes. Given a dictionary of basis functions $\mathcal{B}$, the Main Theorem shows a that the number of individual "pieces" of spheres to approximate a manifold within error threshold $\epsilon > 0$ is lower than that of a purely hyperplane method [@li2019]. If $M$ is a $d$-dimensional, compact, $C^3$-smooth Riemannian manifold, the covering number $\mathcal{N}_{\mathcal{B}}(\epsilon, M)$ is defined as:

$$
N_{\mathcal{B}}(\epsilon, M) := \inf_{N} \left\{N : \exists \{C_k, \Psi_k, B_k \}_{k = 1}^K \, s.t. \, \|x - \Psi(x) \| \le \epsilon, \, \forall x \in M \right\}
$$

Here, $C_k$ defines the $k$th partition of ambient space $\mathbb{R}^D$, $B_k$ is a local basis within the dictionary of basis functions $\mathcal{B}$. We have previously defined $\Psi$ to be the global projection, so $\Psi_k$ is the local projection within the $k$th partition and $\Psi = \sum_{k=1}^K 1_{x \in C_k}\Psi_k$ defines the global projection as a combination of local projections.

This definition of covering number holds that there exists some number $N$ such that we can fit a sphere with error lower than threshold $\epsilon$. In the context of spherical PCA, we have two general choices of $\mathcal{B}$: $\mathcal{S}$---the dictionary of basis functions that use a spherical affine space---and $\mathcal{H}$---the dictionary of basis functions that use hyperplanes. Note that if we allow the construction of spheres with infinite radius, we obtain manifolds with $\kappa = 0$, which are hyperplanes! Thus, $\mathcal{H} \subset \mathcal{S}$. As such, we arrive at the inequality:

$$
N_{\mathcal{S}}(\epsilon, M) \le N_{\mathcal{H}}(\epsilon, M)
$$

Spherical PCA asserts that in the worst case with totally linear manifolds, the algorithm will perform as poorly as local PCA that uses hyperplanes instead of sections of spheres as its dictionary of basis functions.

\subsubsection{Main Theorem}

We can also achieve a tighter upper bound than the above inequality. Given a manifold $M$ that satisfies the conditions listed above ($d$-dimensional with $d << D$, $C^3$-smooth, compact, Riemannian), we define additional notation:

  * $K(p,\vec{v})$ is a function that assigns a curvature of the geodesic on $M$ starting at arbitrary point $p$ with direction $\vec{v}$,
  
  * $K^*$ is the maximum curvature defined above with $K^*:= \sup_{(p,\vec{v}) \in UTM} K(p, \vec{v}) < \infty$,
  
  * Similarly, if $T(p,\vec{v})$ defines the rate of change of curvature, then $T^* := \sup_{(p, \vec{v}) \in UTM} T(p, \vec{v})$ defines the maximum rate of change in curvature,
  
  * $UTM$ as encountered above  defines the unit sphere bundle over $M$ and $UT_pM$ defines the unit sphere bundle centered at point $p \in M$,
  
  * $F_\epsilon$ is the set of $\epsilon$-spherical points on $M$ with difference in maximum and minimum curvature under a threshold level parametrized by $\epsilon$:

$$
\begin{gathered}
F_\epsilon := \left\{ p \in M : \sup_{\vec{v} \in UT_pM} K(p,\vec{v}) - \inf_{\vec{v}\in UT_pM} K(p,\vec{v}) \le \left(\frac{2\epsilon}{K^*}\right)^{1/2} \right\}
\end{gathered}
$$

  * $B(p, \epsilon)$ is a ball with radius $\epsilon$ and center $p$.

That is to say, the spherical submanifold $M_\epsilon$ of $M$ for a given error $\epsilon$ is the union of all the balls centered at points in $F_\epsilon$, or
  
$$
M_\epsilon := \bigcup_{p \in F_\epsilon} B\left(p, \left(\frac{6\epsilon}{3 + T}\right)^{1/3}\right)
$$

We have therefore defined a way to define a spherical submanifold $M_\epsilon$ of base manifold $M$ given an error level $\epsilon$. We define $V_\epsilon$ as the volume contained within this spherical submanifold $M_\epsilon$.

We have now defined the notation necessary for the main theorem, which contrasts covering numbers for hyperplanes and sections of spheres.

**Main Theorem**. [@li2019] For any $\epsilon > 0$ and compact $C^3$ $d$-dimensional Riemannian manifold $M$,

$$
\begin{gathered}
N_{\mathcal{H}} \lesssim V\left(\frac{2\epsilon}{K^*}\right)^{-d/2} \\
N_{\mathcal{S}} \lesssim V_\epsilon\left(\frac{6\epsilon}{3 + T^*}\right)^{-d/3} + (V - V_\epsilon)\left(\frac{2\epsilon}{K^*}\right)^{-d/2}
\end{gathered}
$$

A key result from the Main Theorem is that the covering number of spherelets has a lower upper bound when the difference between $V$ and $V_\epsilon$ is small. In the case when $V - V_\epsilon \rightarrow 0$, the second term in the bound for $N_\mathcal{S}$ shrinks to 0 and the first term dominates, which grows at $O(c^-{\frac{d}{3}})$, which outperforms the $O(c^{\frac{d}{2}})$ that $N_\mathcal{H}$ grows at.

In the case when $V_\epsilon \rightarrow 0$ (when the degree of curvature is low so the spherical submanifold is unable to cover much of the underlying manifold), spherical PCA converges to hyperplane PCA.

According to Li and Dunson, the bounds to the Main Theorem are tight, implying that spherelets often require many fewer pieces than locally linear dictionaries and approximate $M$ for a given error level $\epsilon$. Spherelets provide an exceptionally good approximation of $M$ when there the curvature across the manifold stays in a relatively narrow range, as these would require many more parameters in locally linear PCA to approximate. As such, spherelets provide a more robust way of fitting datasets that have intrinsic curvature in a lower $d$-dimensional manifold compared to hyperplane PCA.

\subsection{Spherical PCA}

\subsubsection{Notation}

Assume we have the $N \times D$ data matrix $\mathbf{X}$, with $N$ observations and $D$ variable where $x_{i,j}$ represents the $i$th observation of the $j$th variable:

$$
\mathbf{X} = \begin{bmatrix}
x_{1,1} & \dots & x_{1,D} \\
\vdots & \ddots & \vdots \\
x_{i,1} & \dots & x_{i,D} \\
\vdots & \ddots & \vdots \\
x_{N,1} & \dots & x_{N,D}
\end{bmatrix}
$$

As in linear PCA, a more succinct way to represent this matrix is to write:

$$
\mathbf{X} = \begin{bmatrix} \vec{x}_1^T \\ \vdots \\ \vec{x}_i
^T\\ \vdots \\ \vec{x}_N^T \end{bmatrix}
$$

We will once again treat $\vec{x}_1, \dots, \vec{x}_N$ as i.i.d. samples of a random vector in $\mathbb{R}^D$ from the same underlying distribution $F$ for which $E_{x \sim F} \|x\|^2 < \infty$ to assure that the mean and covariance of the data matrix are well-deefined. We use:

$$
\overline{x} = \frac{1}{N} \sum_{i = 1}^N \vec{x}_i
$$

to denote the sample mean of $\vec{x}$. To denote the covariance operator, we use:

$$
\mathbf{\Sigma}_x = \frac{1}{N} \sum_{i = 1}^N (\vec{x}_i - \overline{x})(\vec{x}_i - \overline{x})^T = \frac{1}{N} \left(\sum_{i = 1}^N \vec{x}_i \vec{x}_i^T \right) - \overline{x}\overline{x}^T = \frac{1}{N} \mathbf{X}^T \mathbf{X} - \overline{x} \overline{x}^T
$$

This covariance operator will come into use later on. Additional notation includes:

  * $1_N$ is the column vector of all ones with length $N$ (i.e. $1_N \in \mathbb{R}^N$)
  
  * $\| \vec{z} \|$ denotes the Euclidean norm (i.e. for $\vec{z} \in \mathbb{R}^d, \|\vec{z}\| = \left(\sum_{i = 1}^{d} z_i ^2 \right)^{1/2} = \sqrt{\vec{z}^T \vec{z}}$)
  
  * $\Psi: \mathbb{R}^D \rightarrow \mathbb{R}^{d+1}$ is a projection map from space $\mathbb{R}^D$ to $\mathbb{R}^{d+1}$.
    Note: $\Psi$ maps from $\mathbb{R}^D$ to $\mathbb{R}^{d+1}$ rather than $\mathbb{R}^{d}$ because the projected points lie on an affine subspace with one fewer degree of freedom than the manifold. One example is fitting a curve in $\mathbf{R}^2$ with sections of 2-dimensional circles; while circles are 2D, the projected points only live along the edge of the circle. As such, we include on extra dimension in our projection map to account for the extra degree of freedom.
    
  * $d^2(\cdot, \cdot)$ is the distance operator between two points. Note that $d^2(x, y)$ is equivalent to $\sqrt{\| x - y \|}$.

\subsubsection{Assumptions}

There are three main assumptions made by Li and Dunson:

  * **Distributional Assumption**. The data matrix $\mathbf{X} \in \mathbb{R}^{N \times D}$ consists of a transformation $V^* \Lambda^{*\frac{1}{2}} \mathbf{Z}$ where $\mathbf{Z} \in \mathbb{R}^{N \times D}$ is a data matrix consisting of i.i.d $z_i$ non-degenerate random variables.
  * **Moment Assumption**. For the underlying random variables $z_{i}$, $E(z_{i}) = 0$, $E(z_i^2) = 1$, and $E(z_i^6) < \infty$. We can choose an affine transformation $V^*$ such that the underlying random variables satisfy the first two moment assumptions. A similar assumption is considered in Lee et al. that uses bounded fourth moments instead to prove convergence in high dimensional PCA [@lee2010].
  * **Spike Population Model Assumption**. If $\lambda_1, \lambda_2, \dots, \lambda_D$ are the ordered eigenvalues of $\Lambda^*$ as similar to a PCA setting, then there exists an integer $m > d$ such that $\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_m \ge lambda_{m+1} = \dots = \lambda_D = 1$.
  
The consequence of these assumptions is that we have a theoretical guarantee that the estimate projection $\hat{\Psi}$ converges in probability to $\Psi^*$. In other words:

$$
\lim_{n \rightarrow \infty} P(\sup_{x \in M} \| \hat{\Psi}(x) - \Psi^*(x) \| > \epsilon) = 0, \, \forall\epsilon <0
$$

\subsubsection{Estimation}

Given a set of data $\vec{x}_1, \dots, \vec{x}_N \in \mathbb{R}^D$ organized into data matrix $\mathbf{X}$, spherical PCA estimates the best approximating sphere $S_{V}(c,r)$, where $c$ is the center, $r$ is the radius, and $V \in \mathbb{R}^{(d+1) \times (d+1)}$ is the $(d+1)$th dimensional affine subspace the sphere lives on. 

For observation $\vec{x}_i$, the closest point $\vec{y}_i$ lying on the sphere $S_V(c,r)$ is the point that minimizes Euclidean distance $d^2(\vec{x}_i, \vec{y}_i)$ between $\vec{x}$ and $\vec{y}$. 

Continuing the convention that $\Psi$ is an orthogonal projection from $R^D$ to the affine subspace given by c + V, then $\Psi$ is given by:

$$
\hat{\Psi}(\vec{x}_i) = \hat{c} + \frac{\hat{r}}{\| \hat{V}\hat{V}^T (\vec{x}_i - \hat{c}) \|}\hat{V}\hat{V}^T(\vec{x}_i - \hat{c})
$$

Note that $\vec{x} - \Psi_{V,c}(\vec{x}) \perp \Psi_{V,c}(\vec{x}) - \vec{y}$ since $\Psi$ is an orthogonal projection.

The optimal subspace $V$ is given by $\hat{V} = (\vec{v}_1, \dots, \vec{v}_{d+1})$, where $\vec{v}_i, i \in \{1,\dots,d+1\}$ is the $i$th eigenvector ranked in descending order of $(\mathbf{X} - 1_N\bar{\mathbf{X}})^T(\mathbf{X} - 1_N\bar{\mathbf{X}})$. This is a result taken from linear PCA, where we eigendecomposed the covariance matrix $\Sigma$ to obtain the decreasing eigenvalues and eigenvectors to obtain the principal component scores and loadings. In fact, the matrix given by $(\mathbf{X} - 1_N\bar{\mathbf{X}})^T(\mathbf{X} - 1_N\bar{\mathbf{X}})$ is equivalent to:

$$
\begin{aligned}
(\mathbf{X} - 1_N\bar{\mathbf{X}})^T(\mathbf{X} - 1_N\bar{\mathbf{X}})&= \mathbf{X}^T \mathbf{X} - \bar{\mathbf{X}}^T1_N^T\mathbf{X} - \mathbf{X}^T 1_N \bar{\mathbf{X}} + \bar{\mathbf{X}}^T 1_N^T 1_N \bar{\mathbf{X}} \\
&= \mathbf{X}^T \mathbf{X} - N \overline{x} \overline{x}^T \\
&= N \cdot \Sigma_X
\end{aligned}
$$

In other words, the first step of spherical PCA is to eigendecompose the scaled covariance matrix $\Sigma_X$! This is where the "PCA" aspect of spherical PCA arises.

If $\vec{z}_i = \bar{\mathbf{X}} + \hat{V}\hat{V}^T(\vec{x}_i - \bar{\mathbf{X}})$ are a change of basis to affine subspace $V$, then it can be shown that the minimizing pair $(\vec{\eta}^*, \vec{\xi}^*)$ of loss function $g(\vec{\eta}, \vec{\xi}) = \sum_{k = 1}^N(\vec{z}_i^T \vec{z}_i + \vec{\eta}^T \vec{x}_i + \vec{\xi})^2$ is:

$$
\begin{gathered}
\vec{\eta} = -H^{-1}\omega \\
\vec{\xi} = -\frac{1}{N} \sum_{k=1}^N(\vec{z}_i^T \vec{z}_i + \vec{\eta}^T\vec{z}_i)
\end{gathered}
$$

where $H$ and $\omega$ are defined as:

$$
\begin{aligned}
H &= \sum_{i = 1}^N (\vec{z}_i - \overline{z})(\vec{z}_i - \vec{z})^T \\
&= N \Sigma_z \\
\vec{\omega} &= \sum_{i=1}^N \left( \|\vec{z}_i^T\vec{z}_i\| - \frac{1}{N}\sum_{j=1}^N\|\vec{z}_j^T\vec{z}_j\| \right) (\vec{z}_i - \overline{z})
\end{aligned}
$$

The $H$ matrix can be seen as a centered matrix that is the covariance matrix of new coordinates $\mathbf{Z}$ multiplied by number of observations $N$. $\vec{\omega}$ is a vector where $\omega_i$ is the centered $i$th $z$-coordinate scaled by a weight, where the weight is a centered magnitude of the $i$th $z$-coordinate.

The optimal parametrization $(\hat{V}, \hat{c}, \hat{r})$ of the projection of $\mathbf{X} \in \mathbb{R}^{N \times D}$ onto the sphere $S_V(c, r)$ is:

$$
\begin{gathered}
\hat{V} = (\vec{v}_1, \dots, \vec{v}_{d + 1}) \\
\hat{c} = -\frac{\vec{\eta}^*}{2}  \\
\hat{r} = \frac{1}{N} \sum_{k = 1}^N \left\|\vec{z}_i - \hat{c} \right\|
\end{gathered}
$$

The projection map $\hat{\Psi}$ of data matrix $\mathbf{X}$ onto sphere $S_{\hat{V}}(\hat{c}, \hat{r})$ is the projection map onto affine subspace $\hat{c} + \hat{V}$, given by:

$$
\hat{\Psi}(\vec{x}_i) = \hat{c} + \frac{\hat{r}}{\| \hat{V}\hat{V}^T (\vec{x}_i - \hat{c}) \|}\hat{V}\hat{V}^T(\vec{x}_i - \hat{c})
$$

\subsection{Algorithm}

We have now defined spherical PCA (SPCA) to project the data $\mathbf{X}$ down to single sphere $S_V$. In general, this single sphere will typically not be a sufficient approximation for the inherent manifold $M$. Instead, we partition the space $\mathbb{R}^D$ into $k$ disjoint subsets $C_1, \dots, C_k$. 

For the $k$th disjoint subset, we can define a data matrix $\mathbf{X}_k = \{X_i : X_i \in C_k\}$ that is a partition of the original data that lies within $C_k$. After applying SPCA to $\mathbf{X}_k$, we obtain spherical volume, center, and radius $(\hat{V}_k, \hat{c}_k, \hat{r}_k)$ alongside projection map $\Phi_k$ as a map from $x\in C_k$ to $y \in S_{\hat{V}_k}(\hat{c}_k, \hat{r}_k)$. A spherelets estimation $\hat{M}$ of the manifold $M$ can be obtained by setting $\hat{M} = \bigcup_{k = 1}^K \hat{M}_k$, where $\hat{M}_k$ is the local SPCA in the $k$th region and $\hat{M}_k = S_{\hat{V}_k}(\hat{c}_k, \hat{r}_k) \cap C_k$.

We thus arrive at the spherical PCA algorithm. The basic strokes of the spherical PCA process are:

  1. Subdivide the space (and dataset) into $k$ partitions: $\{C_k\}_{k=1}^K$ and $\{M_k\}_{k=1}^K$,
  2. Calculate a submanifold $\hat{M_k}$ and projection map $\hat{\Psi_k}$ for each submanifold,
  3. Find the union of $\{M_k\}_{k=1}^K$ and $\Psi_k$ to obtain estimates $\hat{M}$ and $\Psi$.

The specific algorithm is as follows:

\begin{algorithm}[H]
  \caption{Spherelets}
  \textbf{Input:} Data matrix $\mathbf{X}$; intrinsic dimension $d$; partition $\{C_k\}_{k = 1}^K$ \\
  \textbf{Output:} Local estimated manifolds $\hat{M}_k$ and projection map $\hat{\Psi}_k, k \in \{1, \dots, K\}$; global estimated manifold $\hat{M}$ of intrinsic manifold $M$ and projection map $\Psi$.
  \begin{algorithmic}[1]
    \FOR{(k = 1 : K)}
      \STATE Define $\mathbf{X}_{[k]} = \mathbf{X} \cap C_k$\; \\
      \STATE Calculate $\hat{V}_k, \hat{c}_k, \hat{r}_k$\; \\
      \STATE Calculate $\hat{\Psi}_k(x) = \hat{c}_k + \frac{\hat{r}_k}{\|\hat{V}_k\hat{V}_k^T(x-\hat{c}_k)\|}(x - \hat{c}_k)$\; \\
      \STATE Calculate $\hat{M}_k = S_{\hat{V}_k}(\hat{c}_k, \hat{r}_k) \cap C_k$\;
    \ENDFOR
    \STATE Calculate $\Psi(x) = \sum_{k = 1}^K \mathbf{1}_{\{x \in C_k\}}\hat{\Psi}_k(x)$, and $\hat{M} = \bigcup_{k =1}^K\hat{M}_k$.
  \end{algorithmic}
\end{algorithm}

\section{Discussion}

Previously within this paper, I have incorporated parts of the geometric intuition and connections to linear PCA in other sections but will rehash it briefly in this section as well. Within the Section 2.1, I also discuss the Main Theorem and its theoretical guarantees for the relative performance of spherelets in terms of covering number.

Within this section, I will discuss the various strengths and weaknesses that spherelets provide relative to local linear PCA using hyperplanes.  I will also incorporate extensions and applications of spherelets to other aspects of dimension reduction and data visualization.

\subsection{Strengths}

The two main draws for spherical PCA are its flexibility and its ability to offer out of sample testing without reinitializaiton of the entire algorithm.

  - **High curvature.** As mentioned prior, the motivating desire behind spherical PCA is to approximate manifolds in lower dimensions that other fully linear methods are not able to capture. Spherical PCA projects data down to
  
  - Performs well in areas with high curvature that local PCA can't approximate
  - **Out of sample testing.** Can perform OOS assessments and returns the underlying manifold
  
\subsection{Weaknesses}

While spherical PCA is capable of fitting manifolds that have higher degrees of curvature than linear PCA to a certain extent, spherelets still struggle with areas, sometimes in similar regards to linear PCA. Main disadvantages of spherelets include sensitivity to non-uniform dimensions and curvature along the manifold and computational inefficiency when paired with a poor choice of partitioning algorithm and choice of intrinsic manifold dimensionality $d$.

  - Struggles with areas of non-uniform curvature
  - Struggles with non-uniform dimensions
  - Must specify inherent dimension $d$
  - Computationally expensive
  - Dependent on choice of manifold subsetting

\section{Examples}

To generate numerical examples, I used the `SPCA` and `SS_calc` functions written by co-author Minerva Mukhopadhyay [@github]. The `SPCA` function takes in a matrix of $N$ observations $\vec{x}_i \in \mathbb{R}^{D}, i \in 1, \dots, N$ and returns the error given by spherical and local PCA (`SS` and `SS_new`), as well as the projected values `Y_D`.

```{r example_setup}

partition <- function(data, k) {
  # partitions data
  bin <- floor(nrow(data) / k)
  sto <- vector(mode = "list", length = k)
  for (i in 1:k) {
    if (i == k) {
      sto[[i]] <- data[(1 + (i - 1) * bin) : nrow(data),]
    }
    else {
      sto[[i]] <- data[(1+ (i-1) * bin):(i*bin),]
    }
  }
  return(sto)
}

spca <- function(x, dimension) {
  out <- SPCA(x, dimension)
  ss_out <- SS_calc(X = x, mu = colMeans(x), c = out$c,
                    V = out$V, r = out$r, d = dimension, 
                    c.d = out$c_d)
  return(ss_out)
}
```

\subsection{Euler Spiral}

The Euler spiral is a curve in $\mathbb{R}^1$ that has a curvature that changes linearly with arc length. In other words, $\kappa(s) = s$. The Euler spiral can be parametrized as follows:

$$
\begin{bmatrix}x(s) \\ y(s)\end{bmatrix} = 
\begin{bmatrix}
\displaystyle\int_0^s \cos(t^2) dt \\
\displaystyle\int_0^s \sin(t^2) dt
\end{bmatrix} \hspace{0.1in} s \in [0, 4]
$$

We use the Euler spiral as a demonstration to see how spherical PCA is able to handle regions with curvature.

```{r Euler}
# generating euler spiral data
s <- seq(0, 4, by = 0.001)
data <- matrix(rep(0, 2 * length(s)), ncol = 2)
for (i in 1:length(s)) {
  data[i,1] <- integrate(function(t) sin(t^2),
                     lower = 0,
                     upper = s[i])$value
  data[i,2] <- integrate(function(t) cos(t^2),
                     lower = 0,
                     upper = s[i])$value
}
plot(data, type = "l", lwd = 4, xlab = "", ylab = "")
```


```{r euler, fig.cap='Spherical PCA performed on an Euler spiral with $k = 3, 8$.'}
par(mfrow = c(1, 2), mar = c(2, 2, 1, 1))
euler <- function(k) {
  d <- partition(data, k)
  spca_d <- lapply(d, FUN = function(x) spca(x, 1))
  rainbow_cols <- alpha(sample(rainbow(k)), 0.08)
  plot(data, type = "l", lwd = 4,
       xlab = "", ylab = "")
  for (i in 1:k) {
    points(spca_d[[i]]$Y_D, col = rainbow_cols[i], pch = 19, cex = 0.6)
  }
}

euler(3); euler(10)
```

\subsection{Helix}

```{r helix, fig.cap='Spherical PCA performed on a helix with $k = 3, 8$.'}
# generating helix data
s <- seq(0, 6*pi, by=0.1)
data <- matrix(c(cos(s), sin(s), s), ncol = 3)

# spherical pca on helix
par(mfrow = c(1, 2), mar = c(2, 2, 1, 1))
spiral <- function(k) {
  d <- partition(data, k)
  spca_d <- lapply(d, FUN = function(x) spca(x, 1))
  rainbow_cols <- alpha(sample(rainbow(k)), 0.8)
  spl <- scatterplot3d(data, type = "l", lwd = 4,
                       xlab = "", ylab = "", zlab = "")
  for (i in 1:k) {
    spl$points3d(spca_d[[i]]$Y_D, 
                 col = rainbow_cols[i], pch = 19, cex = 0.6)
  }
}

spiral(3); spiral(8)
```

\subsection{Cylinder}

```{r cylinder, fig.cap='Spherical PCA performed on a cylinder with $k = 3, 8$.'}
N <- 10^3
theta <- runif(N, 0, 2*pi)
data <- matrix(c(cos(theta), sin(theta), sort(runif(N, 0, 5))), 
               ncol = 3)

# spherical pca on cylinder
par(mfrow = c(1, 3), mar = c(1, 1, 1, 1))
cylinder <- function(k, new.plot = F) {
  d <- partition(data, k)
  spca_d <- lapply(d, FUN = function(x) spca(x, 2))
  rainbow_cols <- alpha(sample(rainbow(k)), 0.8)
  if (new.plot) {
    spl <- scatterplot3d(data, color = rgb(0, 0, 0, 0.5),
                       xlab = "", ylab = "", zlab = "", 
                       mar = c(1, 1, 1, 1))
  }
  mse <- c()
  for (i in 1:k) {
    if (new.plot) {
      spl$points3d(spca_d[[i]]$Y_D, 
                 col = rainbow_cols[i], pch = 19, cex = 0.2)
    }
    mse <- c(mse, (spca_d[[i]]$Y_D - d[[i]])^2)
  }
  return(mean(mse))
}

cylinder(3, new.plot = T); cylinder(8, new.plot = T)
vec_cylinder <- Vectorize(cylinder)
plot(vec_cylinder(1:10), main = "MSE", xlab = "k")
```

We see that SPCA is not fully capable of handling a cylinder.

\newpage

\section{References}
